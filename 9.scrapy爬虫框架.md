# scrapy爬虫框架

## 普通爬虫流程

![process](/image/spiderprocess.png)

## scrapy爬虫框架工作流程

![scrapy](/image/scrapyprocess.jpg)

其流程可以描述如下：

* 爬虫中起始的url构造成request对象-->爬虫中间件-->引擎-->调度器
* 调度器把request-->引擎-->下载中间件-->下载器
* 下载器发送请求，获取response响应-->下载中间件-->引擎-->中间件-->爬虫中间件-->爬虫
* 爬虫提取url地址，组装成request对象-->爬虫中间件-->引擎-->调度器,重复步骤二
* 爬虫提取数据-->引擎-->管道处理和保存数据

### scrapy的三个内置对象

* request请求对象：由url method post_data headers等构成
* response响应对象：由url body status headers等构成
* item数据对象：本质是个字典

### scrapy中每个模块的具体作用

![function](/image/scrapyfunction.jpg)

* 引擎：数据和信号的传递
* 调度器：任务队列
* 下载器：下载数据
* 爬虫
  * 起始url
  * 解析
* 管道：保存数据
* 中间件：定制化操作

## scrapy框架的使用

### scrapy框架的安装

```bash
sudo apt-get install scrapy

# 或者
pip install scrapy
```

### scrapy项目开发流程

* 创建项目：
  * scrapy startproject mySpider
* 生成一个爬虫
  * scrapy genspider itcast itcast.cn
* 提取数据
  * 根据网站结构在spider中实现数据采集相关内容
* 保存数据
  * 使用pipline进行数据后续处理和保存


#### 创建项目

创建scrapy项目的命令:

```bash
scrapy startproject 项目名称

# 例如
scrapy startproject myspider
```

生成的目录和文件结构如下：

![project](/image/scrapyproject.PNG)

#### 创建爬虫

命令：
在项目目录下执行

```bash
scrapy genspider <爬虫名字> <允许爬取的域名>
```

* 爬虫名字：作为爬虫运行时的参数
* 允许爬取的域名：对爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果要爬取的url域被允许的域不通，则被过滤掉

示例

```bash
cd myspider

scrapy genspider itcast itcast.cn
```

生成的目录文件结果如下：

![spider](/image/scrapyspider.PNG)

爬虫文件内容介绍：

* 三个参数：
  * name
  * allowed_domains
  * start_urls 设置起始的url，只要设置就好，通常会被自动的创建成请求发送

* 一个方法:
  * parse 解析方法，通常用于起始url对应响应的解析