# scrapy爬虫框架

## 普通爬虫流程

![process](/image/spiderprocess.png)

## scrapy爬虫框架工作流程

![scrapy](/image/scrapyprocess.jpg)

其流程可以描述如下：

* 爬虫中起始的url构造成request对象-->爬虫中间件-->引擎-->调度器
* 调度器把request-->引擎-->下载中间件-->下载器
* 下载器发送请求，获取response响应-->下载中间件-->引擎-->中间件-->爬虫中间件-->爬虫
* 爬虫提取url地址，组装成request对象-->爬虫中间件-->引擎-->调度器,重复步骤二
* 爬虫提取数据-->引擎-->管道处理和保存数据

### scrapy的三个内置对象

* request请求对象：由url method post_data headers等构成
* response响应对象：由url body status headers等构成
* item数据对象：本质是个字典

### scrapy中每个模块的具体作用

![function](/image/scrapyfunction.jpg)

* 引擎：数据和信号的传递
* 调度器：任务队列
* 下载器：下载数据
* 爬虫
  * 起始url
  * 解析
* 管道：保存数据
* 中间件：定制化操作

## scrapy框架的使用

### scrapy框架的安装

```bash
sudo apt-get install scrapy

# 或者
pip install scrapy
```

### scrapy项目开发流程

* 创建项目：
  * scrapy startproject mySpider
* 生成一个爬虫
  * scrapy genspider itcast itcast.cn
* 提取数据
  * 根据网站结构在spider中实现数据采集相关内容
* 保存数据
  * 使用pipline进行数据后续处理和保存


#### 创建项目

创建scrapy项目的命令:

```bash
scrapy startproject 项目名称

# 例如
scrapy startproject myspider
```

生成的目录和文件结构如下：

![project](/image/scrapyproject.PNG)

#### 创建爬虫

命令：
在项目目录下执行

```bash
scrapy genspider <爬虫名字> <允许爬取的域名>
```

* 爬虫名字：作为爬虫运行时的参数
* 允许爬取的域名：对爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果要爬取的url域被允许的域不通，则被过滤掉

示例

```bash
cd myspider

scrapy genspider itcast itcast.cn
```

生成的目录文件结果如下：

![spider](/image/scrapyspider.PNG)

爬虫文件内容介绍：

* 三个参数：
  * name
  * allowed_domains
  * start_urls 设置起始的url，只要设置就好，通常会被自动的创建成请求发送

* 一个方法:
  * parse 解析方法，通常用于起始url对应响应的解析

* **运行scrapy爬虫**

```bash
# scrapy crawl <爬虫名字>

# 显示日志信息
scrapy crawl itcast

# 不显示日志信息
scrapy crawl itcast --nolog
```

#### 完善爬虫

代码如下：

```python
import scrapy


class ItcastSpider(scrapy.Spider):
    name = 'itcast'
    # 检查域名
    allowed_domains = ['itcast.cn']
    # 修改起始url
    start_urls = ['http://www.itcast.cn/channel/teacher.shtml#ajavaee']

    def parse(self, response):
        # 获取所有教师结点
        node_list = response.xpath('//div[@class="main_mask"]')
        # print(len(node_list))

        for node in node_list:
            temp = {}

            # xpath方法返回的是选择器对象列表,用extract()来提取数据
            # xpath返回的列表只有一个值，可以使用extract_first()，否则使用extract()
            temp['name'] = node.xpath('./h2/text()')[0].extract() # 或者node.xpath('./h2/text()').extract_first()
            temp['title'] = node.xpath('./h2/span/text()')[0].extract()
            temp['entry_time'] = node.xpath('./h3/text()')[0].extract()
            temp['desc'] = node.xpath('./p/text()')[0].extract()
            # 因为要向引擎返回数据，所以不能直接return，用yield来处理
            yield temp
```

**注意**：

* scrapy.Spider爬虫类必须有parse()方法
* 如果网站结构复杂，可以自定义其它解析方法
* 在解析函数中提取的url要发送请求，必须属于allowed_domains范围内，但是start_url中的url地址不受这个限制
* 启动爬虫的时候要注意，是在爬虫的项目路径下启动
* parse()函数中用yield返回数据，**注意：解析函数中yield返回的对象只能是：Baseitem，Request, dict,None**

#### 定位元素以及提取数据和属性值的方法

* response.xpath返回的结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法

* 额外的方法 extract()：返回一个包含有字符串的列表

* 额外的方法 extract_first()：返回第一个字符串，列表为空时返回None

#### response响应对象的常用属性

* response.url：当前响应的url地址
* response.request.url：当前响应对应的请求的url地址
* response.headers：响应头
* response.requests.headers：当前响应的请求头
* response.body：响应体，HTML代码，byte类型
* response.status：响应状态码

### 保存数据

> 利用管道pipeline来处理保存数据

#### 在pipelines.py文件中定义对数据的操作

1. 定义一个管道类
2. 重写管道类的process_item方法
3. process_item方法处理完item之后必须返回给引擎

代码示例：

```python
from itemadapter import ItemAdapter
import json


class MyspiderPipeline:
    def __init__(self):
        self.file = open("itcast.josn", 'w')

    def process_item(self, item, spider):
        # 将字典数据序列化
        # ,号把json数据隔开，\n换行，ensure_ascii=False禁用ascii编码
        json_data = json.dumps(item, ensure_ascii=False) + ',\n'
        # 将数据写入文件
        self.file.write(json_data)
        # 默认使用完管道之后，需要将数据返回给引擎
        return item

    def __del__(self):
        self.file.close()
```

#### 在settings.py配置中启用管道

```python
ITEM_PIPELINES = {
   'myspider.pipelines.MyspiderPipeline': 300,
}
```

* 配置项中键为使用的管道类，管道类使用.进行分割，第一个为项目目录，第二个为文件，第三个为定义的管道类

* 配置项中值为管道的使用顺序，设置的值越小越优先执行，该值一般设置为1000以内

* **运行scrapy爬虫**

```bash
# scrapy crawl <爬虫名字>

# 显示日志信息
scrapy crawl itcast

# 不显示日志信息
scrapy crawl itcast --nolog
```

### scrapy数据建模与请求

#### 数据建模

 > 通常在做项目的过程中，在items.py中进行数据建模

##### 为什么建模

1. 定义item即提前规划好哪些字段需要抓取，防止手误，因为在定义好之后，系统会自动检查

2. 配合注释一起可以清晰的知道要抓取哪些字段，没有定义的字段不能抓取，字段少的时候可以用字代替

3. 使用scrapy的一些特定组件的需要Item做支持，如scrapy的imagesPipeline管道类

##### 如何建模

在items.py文件中定义需要提取的字段：

```python
class MyspiderItem(scrapy.Item):
    name = scrapy.Filed()  # 讲师名字
    title = scrapy.Filed() # 讲师的职称
    desc = scrapy.Filed()  # 讲师的介绍
```

##### 如何使用模板类

模板类定义以后需要在爬虫中导入并实例化，之后的使用方法和使用字典相同

示例代码：

```python
from myspider.items import MyspiderItem

    def parse(self, response):
        # 获取所有教师结点
        node_list = response.xpath('//div[@class="main_mask"]')
        # print(len(node_list))

        for node in node_list:
            # temp = {}
            item = MyspiderItem()

            # xpath方法返回的是选择器对象列表,用extract()来提取数据
            # xpath返回的列表只有一个值，可以使用extract_first()，否则使用extract()
            item['name'] = node.xpath('./h2/text()')[0].extract() # 或者node.xpath('./h2/text()').extract_first()
            item['title'] = node.xpath('./h2/span/text()')[0].extract()
            item['entry_time'] = node.xpath('./h3/text()')[0].extract()
            item['desc'] = node.xpath('./p/text()')[0].extract()
            # 因为要向引擎返回数据，所以不能直接return，用yield来处理
            yield item
``` 

#### 开发流程总结：

1. 创建项目

scrapy startproject 项目名

2. 明确目标

在items.py文件中进行建模

3. 创建爬虫
  3.1 创建爬虫
  scrapy genspider 爬虫名 允许的
  3.2 完成爬虫
  修改start_urls
  检查修改allowed_domains
  编写解析方法

4. 保存数据

在pipelines.py文件中定义对数据处理的管道
在settings.py文件中注册启用管道

#### 翻页请求的思路

> 对于要提取如下图中所有页面上的数据该怎么办呢？

![page](/image/turnpage.PNG)

回顾requests模块是如何实现翻页请求的：

```txt
1.找到下一页的URL地址
2.调用requests.get(url)
```

scrapy实现翻页的思路：

```txt
1.找到下一页的URL地址
2.构造URL地址的请求对象，传递给引擎
```

#### 构造Request对象，并发送请求

##### 实现方法

1. 确定url地址
2. 构造请求，scrapy.Request(url, callback)
    * callback：指定解析函数名称，表示该请求返回的响应使用哪个函数进行解析

3. 把请求交给引擎：yield scrapy.Request(url, callback)