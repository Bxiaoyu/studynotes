# scrapy爬虫框架

## 普通爬虫流程

![process](/image/spiderprocess.png)

## scrapy爬虫框架工作流程

![scrapy](/image/scrapyprocess.jpg)

其流程可以描述如下：

* 爬虫中起始的url构造成request对象-->爬虫中间件-->引擎-->调度器
* 调度器把request-->引擎-->下载中间件-->下载器
* 下载器发送请求，获取response响应-->下载中间件-->引擎-->中间件-->爬虫中间件-->爬虫
* 爬虫提取url地址，组装成request对象-->爬虫中间件-->引擎-->调度器,重复步骤二
* 爬虫提取数据-->引擎-->管道处理和保存数据

### scrapy的三个内置对象

* request请求对象：由url method post_data headers等构成
* response响应对象：由url body status headers等构成
* item数据对象：本质是个字典

### scrapy中每个模块的具体作用

![function](/image/scrapyfunction.jpg)

* 引擎：数据和信号的传递
* 调度器：任务队列
* 下载器：下载数据
* 爬虫
  * 起始url
  * 解析
* 管道：保存数据
* 中间件：定制化操作

## scrapy框架的使用

### scrapy框架的安装

```bash
sudo apt-get install scrapy

# 或者
pip install scrapy
```

### scrapy项目开发流程

* 创建项目：
  * scrapy startproject mySpider
* 生成一个爬虫
  * scrapy genspider itcast itcast.cn
* 提取数据
  * 根据网站结构在spider中实现数据采集相关内容
* 保存数据
  * 使用pipline进行数据后续处理和保存


#### 创建项目

创建scrapy项目的命令:

```bash
scrapy startproject 项目名称

# 例如
scrapy startproject myspider
```

生成的目录和文件结构如下：

![project](/image/scrapyproject.PNG)

#### 创建爬虫

命令：
在项目目录下执行

```bash
scrapy genspider <爬虫名字> <允许爬取的域名>
```

* 爬虫名字：作为爬虫运行时的参数
* 允许爬取的域名：对爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果要爬取的url域被允许的域不通，则被过滤掉

示例

```bash
cd myspider

scrapy genspider itcast itcast.cn
```

生成的目录文件结果如下：

![spider](/image/scrapyspider.PNG)

爬虫文件内容介绍：

* 三个参数：
  * name
  * allowed_domains
  * start_urls 设置起始的url，只要设置就好，通常会被自动的创建成请求发送

* 一个方法:
  * parse 解析方法，通常用于起始url对应响应的解析

* **运行scrapy爬虫**

```bash
# scrapy crawl <爬虫名字>

# 显示日志信息
scrapy crawl itcast

# 不显示日志信息
scrapy crawl itcast --nolog
```

#### 完善爬虫

代码如下：

```python
import scrapy


class ItcastSpider(scrapy.Spider):
    name = 'itcast'
    # 检查域名
    allowed_domains = ['itcast.cn']
    # 修改起始url
    start_urls = ['http://www.itcast.cn/channel/teacher.shtml#ajavaee']

    def parse(self, response):
        # 获取所有教师结点
        node_list = response.xpath('//div[@class="main_mask"]')
        # print(len(node_list))

        for node in node_list:
            temp = {}

            # xpath方法返回的是选择器对象列表,用extract()来提取数据
            # xpath返回的列表只有一个值，可以使用extract_first()，否则使用extract()
            temp['name'] = node.xpath('./h2/text()')[0].extract() # 或者node.xpath('./h2/text()').extract_first()
            temp['title'] = node.xpath('./h2/span/text()')[0].extract()
            temp['entry_time'] = node.xpath('./h3/text()')[0].extract()
            temp['desc'] = node.xpath('./p/text()')[0].extract()
            # 因为要向引擎返回数据，所以不能直接return，用yield来处理
            yield temp
```

**注意**：

* scrapy.Spider爬虫类必须有parse()方法
* 如果网站结构复杂，可以自定义其它解析方法
* 在解析函数中提取的url要发送请求，必须属于allowed_domains范围内，但是start_url中的url地址不受这个限制
* 启动爬虫的时候要注意，是在爬虫的项目路径下启动
* parse()函数中用yield返回数据，**注意：解析函数中yield返回的对象只能是：Baseitem，Request, dict,None**

#### 定位元素以及提取数据和属性值的方法

* response.xpath返回的结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法

* 额外的方法 extract()：返回一个包含有字符串的列表

* 额外的方法 extract_first()：返回第一个字符串，列表为空时返回None

#### response响应对象的常用属性

* response.url：当前响应的url地址
* response.request.url：当前响应对应的请求的url地址
* response.headers：响应头
* response.requests.headers：当前响应的请求头
* response.body：响应体，HTML代码，byte类型
* response.status：响应状态码

### 保存数据

> 利用管道pipeline来处理保存数据

#### 在pipelines.py文件中定义对数据的操作

1. 定义一个管道类
2. 重写管道类的process_item方法
3. process_item方法处理完item之后必须返回给引擎

代码示例：

```python
from itemadapter import ItemAdapter
import json


class MyspiderPipeline:
    def __init__(self):
        self.file = open("itcast.josn", 'w')

    def process_item(self, item, spider):
        # 将字典数据序列化
        # ,号把json数据隔开，\n换行，ensure_ascii=False禁用ascii编码
        json_data = json.dumps(item, ensure_ascii=False) + ',\n'
        # 将数据写入文件
        self.file.write(json_data)
        # 默认使用完管道之后，需要将数据返回给引擎
        return item

    def __del__(self):
        self.file.close()
```

#### 在settings.py配置中启用管道

```python
ITEM_PIPELINES = {
   'myspider.pipelines.MyspiderPipeline': 300,
}
```

* 配置项中键为使用的管道类，管道类使用.进行分割，第一个为项目目录，第二个为文件，第三个为定义的管道类

* 配置项中值为管道的使用顺序，设置的值越小越优先执行，该值一般设置为1000以内

* **运行scrapy爬虫**

```bash
# scrapy crawl <爬虫名字>

# 显示日志信息
scrapy crawl itcast

# 不显示日志信息
scrapy crawl itcast --nolog
```

### scrapy数据建模与请求

#### 数据建模

 > 通常在做项目的过程中，在items.py中进行数据建模

##### 为什么建模

1. 定义item即提前规划好哪些字段需要抓取，防止手误，因为在定义好之后，系统会自动检查

2. 配合注释一起可以清晰的知道要抓取哪些字段，没有定义的字段不能抓取，字段少的时候可以用字代替

3. 使用scrapy的一些特定组件的需要Item做支持，如scrapy的imagesPipeline管道类

##### 如何建模

在items.py文件中定义需要提取的字段：

```python
class MyspiderItem(scrapy.Item):
    name = scrapy.Filed()  # 讲师名字
    title = scrapy.Filed() # 讲师的职称
    desc = scrapy.Filed()  # 讲师的介绍
```

##### 如何使用模板类

模板类定义以后需要在爬虫中导入并实例化，之后的使用方法和使用字典相同

示例代码：

```python
from myspider.items import MyspiderItem

    def parse(self, response):
        # 获取所有教师结点
        node_list = response.xpath('//div[@class="main_mask"]')
        # print(len(node_list))

        for node in node_list:
            # temp = {}
            item = MyspiderItem()

            # xpath方法返回的是选择器对象列表,用extract()来提取数据
            # xpath返回的列表只有一个值，可以使用extract_first()，否则使用extract()
            item['name'] = node.xpath('./h2/text()')[0].extract() # 或者node.xpath('./h2/text()').extract_first()
            item['title'] = node.xpath('./h2/span/text()')[0].extract()
            item['entry_time'] = node.xpath('./h3/text()')[0].extract()
            item['desc'] = node.xpath('./p/text()')[0].extract()
            # 因为要向引擎返回数据，所以不能直接return，用yield来处理
            yield item
``` 

#### 开发流程总结：

1. 创建项目

scrapy startproject 项目名

2. 明确目标

在items.py文件中进行建模

3. 创建爬虫
  3.1 创建爬虫
  scrapy genspider 爬虫名 允许的
  3.2 完成爬虫
  修改start_urls
  检查修改allowed_domains
  编写解析方法

4. 保存数据

在pipelines.py文件中定义对数据处理的管道
在settings.py文件中注册启用管道

#### 翻页请求的思路

> 对于要提取如下图中所有页面上的数据该怎么办呢？

![page](/image/turnpage.PNG)

回顾requests模块是如何实现翻页请求的：

```txt
1.找到下一页的URL地址
2.调用requests.get(url)
```

scrapy实现翻页的思路：

```txt
1.找到下一页的URL地址
2.构造URL地址的请求对象，传递给引擎
```

#### 构造Request对象，并发送请求

##### 实现方法

1. 确定url地址
2. 构造请求，scrapy.Request(url, callback)
    * callback：指定解析函数名称，表示该请求返回的响应使用哪个函数进行解析

3. 把请求交给引擎：yield scrapy.Request(url, callback)

##### scrapy.Request的更多参数

> scrapy.Request(url[,callback, method="GET", headers, body, cookies, meta, dont_filter=False])

参数解释：

* 中括号里的参数为可选参数
* callback：表示当前的url的响应交给哪个函数去处理
* method：指定GET或POST请求
* headers：接收一个字典，其中不包括cookies
* body：接收json字符串，为POST的数据，发送payload_post请求时使用
* cookies：接收一个字典，专门放置cookies
* meta：实现数据在不同的解析函数中传递，常用于一个数据分散在不同结构的时候，meta默认带有部分数据，比如下载延迟，请求深度等
* dont_filter：默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以设置为True，比如贴吧的翻页请求，页面的数据总是在变化；start_url中的地址会被重复请求，否则程序不会启动

**提示：** url，callback，meta这三个最常用

#### meta参数的使用

在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：

```python
def parse(self, response):
    ...
    yield scrapy.Request(detail_url, callback=self.parse_detail, meta={"item":item})

def parse_detail(self, response):
    # 获取之前传入的item
    item = response.meta["item"]
```

**特别注意：**

1. meta参数是一个字典
2. meta字典中有一个固定的键proxy，表示代理ip

### scrapy模拟登陆

> **学习目标**
> 1. 应用 请求对象cookies参数的使用
> 2. 了解start_requests函数的作用
> 3. 应用 构造并发送post请求

#### 回顾之前的模拟登录方法

##### requests模块是如何实现模拟登录的呢？

```python
1. 直接携带cookies请求页面

2. 找url地址，发送post请求存储cookie
```

##### selenium是如何模拟登陆的呢？

```python
1. 找到对应的input标签，输入文本点击登陆
```

##### scrapy的模拟登陆

```python
1. 直接携带cookies

2. 找url地址，发送post请求存储cookie
```

#### scrapy携带cookies直接获取需要登陆后的页面

**应用场景**

> 1. cookie过期时间很长，常见于一些不规范的网站
> 2. 能在cookie过期之前把所有数据拿到
> 3. 配合其他程序使用，比如使用selenium把登陆后的cookie保存到本地，scrapy发送请求之前先读取本地的cookie

##### 实现：重构scrapy的start_requests方法

1. scrapy中的start_url是通过start_requests来进行处理的；
2. 对应的，如果start_url地址中的url是需要登陆后才能访问的url地址，则需要重写start_requests方法并在其中手动添加上cookie

**示例：携带cookies登陆github**

```python
import scrapy


class Git1Spider(scrapy.Spider):
    name = 'git1'
    allowed_domains = ['github.com']
    start_urls = ['http://github.com/exile-morganna']

    def start_requests(self):
        url = self.start_urls[0]
        temp = "finger=1571944565; " \
               "_uuid=AE4C631A-2D3B-7DB7-6B25-DC7F1DB86AFC53294infoc; " \
               "buvid3=71221713-D985-4561-8037-6E9040B6F23070403infoc; " \
               "sid=6a4ami2l; DedeUserID=315883985; DedeUserID__ckMd5=206216d00a419151; " \
               "SESSDATA=260b77d4%2C1611064869%2C0cbee*71; bili_jct=6f47d9a661f7d9beb86a769be06b94ec; " \
               "CURRENT_FNVAL=16; rpdid=|(J~J)kRJ)uJ0J'ulmlluJuku; bp_video_offset_315883985=415220842424899150; " \
               "bp_t_offset_315883985=415220842424899150"

        cookies = {data.split("=")[0]:data.split("=")[-1] for data in temp.split("; ")}
        yield scrapy.Request(
            url=url,
            callback=self.parse,
            cookies=cookies
        )

    def parse(self, response):
        print(response.xpath('/html/head/title/text()').extract_first())
```

**注意：**

1. scrapy中的cookie不能放在headers中，在构造请求的时候有专门的cookie参数，能够接收字典形式的cookie
2. 在setting中设置ROBOTS协议，USER_AGENT

#### scrapy.Request发送post请求

> 从前面的知识知道可以通过srcapy.Request()指定method，body参数来发送post请求，但通常使用scrapy.FormRequest()来发送post请求

##### 发送post请求

注意：srcapy.Request()能够发送表单和Ajax请求

**思路分析：**

1. 找到post的url地址：点击登陆按钮进行抓包，然后定位url地址为https://xxxx.com/session
2. 找到请求体的规律：分析post请求的请求体，其中包含的参数均在前一次的响应中
3. 是否登陆成功：通过请求个人主页，观察是否包含用户名，此处以github为例

示例代码见gitlogin/git2.py


#### scrapy管道的使用

##### 1.pipeline中常用的方法：

* process_item(self, item, spider):
  * 管道类中必须有的函数
  * 实现对item数据的处理
  * 必须return item

* open_spider(self, spider)：在爬虫开启的时候仅执行一次

* close_spider(self, spider)：在爬虫关闭的时候仅执行一次

##### 2.管道问价的修改：

> 通过wangyi爬虫来展示，具体代码见wangyi项目

##### 3.开启管道：

> 为什么要开启多个管道呢？

* 不同的pipeline可以处理不同爬虫的数据，通过spider.name属性来区分
* 不同的pipeline能够对一个或多个爬虫进行不同的数据处理操作，比如一个进行数据清洗，一个进行数据保存
* 同一个管道类也可以处理不同爬虫的数据，通过spider.name属性来区分

```python
ITEM_PIPELINES = {
   'wangyi.pipelines.WangyiPipeline': 300,
   'wangyi.pipelines.WangyiSimplePipeline': 301,
   'wangyi.pipelines.MongoPipeline': 302,
}
```

##### 4.pipeline使用注意点

1. 使用之前需要在settings中开启
2. pipeline在settings中键表示位置（即pipelien在项目中的位置可以自定义），值表示距离引擎的远近，越近数据越先经过：**权重值小的优先执行**
3. 有多个pipeline的时候，process_item的方法必须return item，否则后一个pipeline取到的值为None
4. pipeline中的process_item方法必须有，否则item没有办法接受和处理
5. process_item方法接收item和spider，其中spider表示当前传递item过来的spider
6. open_spider(spider)：能够在爬虫开启的时候执行一次
7. close_spider(spider)：能够在爬虫关闭的时候执行一次
8. open_spider()和close_spider()方法经常用于爬虫和数据库的交互，再爬虫开启的时候建立和数据库的连接，在爬虫关闭的时候断开和数据库的连接


### crawlspider爬虫类

> crawlspider继承自Spider爬虫类，比较复杂

#### crawlspider特点

自动根据规则提取链接并且发送给引擎

#### crawlspider使用

1. 创建crawlspider爬虫
   * scrapy genspider -t crawl name domains