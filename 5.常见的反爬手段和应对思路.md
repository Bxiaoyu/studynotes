# 常见的反爬手段和解决思路

> **学习目标**
> 1.了解服务器反爬原因
> 2.了解服务器常反什么样的爬虫
> 3.了解反爬虫领域常见的一些概念
> 4.了解反爬虫的三个方向
> 5.了解常见基于身份识别进行反爬
> 6.了解常见基于爬虫行为进行反爬
> 7.了解常见基于数据加密进行反爬

## 1.服务器反爬原因

* 爬虫占总PV（指页面的访问次数）比较高，这样浪费钱（尤其是三月份爬虫）
三月份爬虫是什么呢？每年的三月份我们会迎接一次爬虫高峰期，有大量的硕士写论文的时候会选择爬取一些网站，进行舆情分析，因为五月份交论文。

* 公司可免费查询的资源被批量抓走，丧失竞争力，这样赚钱少

* 状告爬虫成功几率小

## 2.服务器常反什么样的爬虫

* 十分低级的应届毕业生写的爬虫

* 十分低级的创业小公司

* 由于程序bug产生的失控爬虫

* 成型的商业对手

* 抽风的搜索引擎

## 3.反爬虫领域常见的一些概念

因为反爬虫暂时是个较新的领域，因此有些定义要自己下：

* 爬虫：使用任何技术手段，批量获取网站信息的一种方式，关键在于批量

* 反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式，关键也在于批量

* 误伤：在反爬过程中，误将普通用户识别为爬虫。误伤概率高的反爬策略，效果再好也不能用

* 拦截：成功的阻止爬虫访问；这里会有拦截率的概念，通常来说，拦截率越高的反爬策略，误伤的可能性就越高，因此需要做个权衡

* 资源：机器成本和人力成本的总和

## 4.反爬的三个方向

* 基于身份识别进行反爬

* 基于爬虫行为进行反爬

* 基于数据加密进行反爬

## 5.常见基于身份识别进行反爬

### 1.通过headers字段来反爬

> headers中有很多字段，这些字段都有可能会被对方服务器拿过来进行判定

**1.1 通过headers中的User-Agent字段来反爬：**

* 反爬原理：爬虫默认情况下不带User-Agent，而是使用模块默认设置

* 解决方法：请求头添加User-Agent字段；更好的方法是使用User-Agent池（收集一堆User-Agent的方法，或是随机生成User-Agent）

**1.2 通过referer字段或者是其它字段来反爬：**

* 反爬原理：爬虫默认情况下不带referer字段，服务器端通过判断请求的发起源头，以此判定是否合法

* 解决方法：请求头添加referer字段

**1.3 通过cookie反爬：**

* 反爬原理：通过检查cookies来查看发起请求的用户是否具备相应的权限，以此来反爬

* 解决方法：进行模拟登陆，成功获取cookies后进行数据爬取

### 2.通过请求参数来反爬

> 服务器可以通过检查请求参数是否正确来判断是否为爬虫

**2.1 通过从html静态文件中获取请求参数：**

* 反爬原理：通过增加获取请求参数的难度进行反爬

* 解决方法：仔细分析抓包得到的每一个包，搞清楚请求之间的联系

**2.2 通过发送请求获取请求数据：**

* 反爬原理：通过增加获取请求参数的难度进行反爬

* 解决方法：仔细分析抓包得到的每一个包，搞清楚请求之间的联系，搞清楚请求参数的来源

**2.3 通过js生成请求参数：**

* 反爬原理：js生成请求参数

* 解决方法：仔细分析js，观察加密的实现过程，通过js2py获取执行结果，或是使用selenium来实现

**2.4 通过验证码来反爬：**

* 反爬原理：对方服务器通过弹出验证码强制验证用户浏览行为

* 解决方法：打码平台或是机器学习的方法来识别验证码，其中打码平台廉价易用，更值得推荐

## 6.常见基于爬虫行为进行反爬

### 1.基于请求频率或请求数量

> 爬虫的行为与普通用户的行为有明显的差别，爬虫的请求频率与请求次数远高于普通用户

**1.1 通过请求ip/账号单位时间内总请求数量进行反爬：**

* 反爬原理：同一个ip/账号大量请求对方服务器资源，有更大可能被识别为爬虫

* 解决方法：对应的购买高质量的ip的方式能解决问题/购买多个账号

**1.2 通过同一ip/账号请求之间的间隔进行反爬：**

* 反爬原理：爬虫前后两个请求之间的时间间隔通常比固定时间间隔短，因此可以用来反爬

* 解决方法：请求之间进行随机等待，模拟真实用户操作，在添加时间间隔之后，为了能高速获取数据，尽量使用代理池，如果是账号，则将账号请求之间设置为随机休眠

**1.3 通过对请求ip/账号每天请求次数设置阈值进行反爬：**

* 反爬原理：正常的浏览行为，其一天的请求次数是有限的，通常超过某一个值，服务器会拒绝

* 解决方法：对应的购买高质量的ip的方式能解决问题/购买多个账号，同时设置请求间随机休眠

### 2.根据爬取行为进行反爬，通常在爬取步骤上做分析

**2.1 通过js实现跳转来反爬：**

* 反爬原理：js实现页面跳转，无法在源码中获取下一页

* 解决方法：多次抓包分析url，分析规律

**2.2 通过蜜罐（陷阱）获取爬虫ip（或者代理ip），进行反爬：**

* 反爬原理：在爬虫获取链接进行请求的过程中，爬虫根据正则，xpath，css等方式进行后续链接的提取，此时服务器可以设置一个陷阱url，会被提取规则获取，但是正常用户无法获取，这样就能有效区分爬虫和正常用户

* 解决方法：使用代理pi批量爬取测试/仔细分析响应的内容结构，找出页面中存在的陷阱

**2.3 通过假数据反爬：**

* 反爬原理：向返回的响应数据中添加假数据污染数据库，通常假数据不会被正常用户看到

* 解决方法：长期运行，核对数据库中数据同实际页面数据的对应情况，如果存在问题，仔细分析响应内容

**2.4 阻塞任务队列：**

* 反爬原理：通过生成大量垃圾url，从而阻塞任务队列，降低爬虫的实际工作效率

* 解决方法：观察运行过程中请求响应状态，仔细分析源码获取垃圾url生成规则，对url进行过滤

**2.5 阻塞网络IO：**

* 反爬原理：发送请求获取响应的过程其实就是一个下载过程，在任务队列中混入一个大文件url，当爬虫进行该请求时会占用网络IO，如果是有多线程则会占用多线程

* 解决方法：观察爬虫运行状态/多线程对请求线程计时/发送请求钱

**2.6 运维平台综合审计：**

* 反爬原理：通过运维平台进行管理，通常采用复合型反爬虫策略，多种手段同时使用

* 解决方法：仔细观察分析，长期运行测试目标网站，检查数据采集速度，多方面处理

## 7.常见基于数据加密进行反爬

### 1.对响应中含有的数据进行特殊化处理

> 通常特殊化处理主要指的就是css数据偏移/自定义字体/数据加密/数据图片/特殊编码格式等

**1.1 通过自定义字体来反爬：**

如猫眼电影评分：
![自定义字体反爬](/image/defineFont.png)

* 反爬原理：使用自有字体

* 解决方法：切换到手机版/解析字体文件进行翻译

**1.2 通过css来反爬：**

如去哪网机票价格：
![css反爬](/image/cssmove.png)

* 反爬原理：源码数据不为真实数据，需要通过css位移才能产生真实数据

* 解决方法：计算css偏移

**1.3 通过js动态生成数据进行反爬：**

* 反爬原理：通过js动态生成

* 解决方法：解析关键js，获得数据生成流程，模拟生成数据

**1.4 通过数据图片化反爬：**

* 反爬原理：数据做成图片的形式（如58同城短租）

* 解决方法：通过使用图片解析引擎从图片中解析数据

**1.5 通过编码格式进行反爬：**

* 反爬原理：不使用默认编码格式，在获取响应之后通常爬虫使用utf-8进行解码，此时解码结果会乱码或报错

* 解决方法：根据源码进行多格式解码，获得真正的解码格式

## 小结

掌握反爬的方式和应对思路