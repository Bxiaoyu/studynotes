# scrapy中间件的使用

> **学习目标：**
> 1. 应用scrapy中间件使用随机UA方法
> 2. 应用scrapy中使用代理ip的方法
> 3. 应用scrapy与selenium配合使用

## 1. scrapy中间件的分类和作用

**1.1 scrapy中间件的分类**

根据scrapy运行流程中所在位置不同分为：

1. 下载中间件
2. 爬虫中间件

**1.2 scrapy中间件的作用：预处理request和response对象**

1. 对header以及cookie进行更换以及处理
2. 使用代理ip
3. 对请求进行定制化操作

在scrapy默认的情况下，两种中间件都在middlewares.py一个文件中
爬虫中间件使用方法和下载中间件相同，且功能重复，***通常使用下载中间件***

## 2. 下载中间件的使用方法

> 编写一个Downloader Middlewares和我们编写一个pipeline一样，定义一个类，然后在settings中开启

Downloader Middlewares的默认方法：

* process_request(self, request, spider):
  * 当每个request通过下载中间件的时候，该方法会被调用
  * 返回None值：没有return也是返回None值，该request对象传递给下载器，或通过引擎传递给其它权重低的process_request方法
  * 返回Response对象：不再请求，把response返回给引擎
  * 返回Request对象：把request对象通过引擎交给调度器对象，此时将不再通过其它权重低的process_request方法

* process_response(self, request, response, spider):
  * 当下载器完成http请求，传递响应给引擎的时候调用
  * 返回Response对象：通过引擎交给爬虫处理或交给权重更低的其它下载器中间件的process_response方法
  * 返回Request对象：通过引擎交给调度器继续请求，此时将不再通过其它权重低的process_request方法

* 在settings.py中配置开启中间件，权重越小越先执行

## 3. 定义实现随机User-Agent的下载中间件

### 3.1 在middlewares.py中完善代码

```python
import random
from scrapy import signals
from Douban.settings import USER_AGENT_LIST

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class RandomUserAgent(object):

    def process_request(self, request, spider):
        # print(request.headers)
        ua = random.choice(USER_AGENT_LIST)
        request.headers['User-Agent'] = ua
```

### 3.2 在settings.py中设置开启自定义的下载插件，设置方法同管道

```python
DOWNLOADER_MIDDLEWARES = {
   # 'Douban.middlewares.DoubanDownloaderMiddleware': 543,
    'Douban.middlewares.RandomUserAgent': 543,
}
```

### 3.3 在settings中添加UA列表

```python
# USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'

USER_AGENT_LIST = ['Mozilla/5.0 (compatible; U; ABrowse 0.6; Syllable) AppleWebKit/420+ (KHTML, like Gecko)',
           'Mozilla/5.0 (compatible; U; ABrowse 0.6;  Syllable) AppleWebKit/420+ (KHTML, like Gecko)',
           'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser 1.98.744; .NET CLR 3.5.30729)',
           'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser 1.98.744; .NET CLR   3.5.30729)',
           'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0;   Acoo Browser; GTB5; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;   SV1) ; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)',
           'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; SV1; Acoo Browser; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; Avant Browser)',
           'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1;   .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)',
           'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; GTB5; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; Maxthon; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)',
           'Mozilla/4.0 (compatible; Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser 1.98.744; .NET CLR 3.5.30729); Windows NT 5.1; Trident/4.0)',
           'Mozilla/4.0 (compatible; Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; GTB6; Acoo Browser; .NET CLR 1.1.4322; .NET CLR 2.0.50727); Windows NT 5.1; Trident/4.0; Maxthon; .NET CLR 2.0.50727; .NET CLR 1.1.4322; InfoPath.2)',
           'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser; GTB6; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)'
           ]
```

运行爬虫观察现象

## 4. 代理ip的使用

### 4.1 思路分析

1. 代理添加的位置：request.meta中添加proxy字段
2. 获取一个代理ip，赋值给request.meta['proxy']
   * 代理池中随机选择代理ip
   * 代理ip的webapi发送请求获取一个代理ip

### 4.1 在middlewares.py中完善代码

```python
import random
import base64
from scrapy import signals
from Douban.settings import USER_AGENT_LIST, PROXY_LIST


class RandomProxy(object):

    def process_request(self, request, spider):
        proxy = random.choice(PROXY_LIST)

        if 'user_passwd' in proxy:
            # 对帐号密码进行编码
            b64_up = base64.b64encode(proxy['user_passwd'].encode()) # 传入的是bytes类型
            # 设置认证
            request.headers['proxy-Authorization'] = 'Basic ' + b64_up.decode() # Basic后面要跟空格
            # 设置代理
            request.meta['proxy'] = proxy['ip_port']
        else:
            # 设置代理
            request.meta['proxy'] = proxy['ip_port']
```

### 4.2 在settings.py中设置开启自定义的下载插件，设置方法同管道

```python
DOWNLOADER_MIDDLEWARES = {
   # 'Douban.middlewares.DoubanDownloaderMiddleware': 543,
    #'Douban.middlewares.RandomUserAgent': 543,
    'Douban.middlewares.RandomProxy': 543,
}
```

### 3.3 在settings中添加Proxy列表

```python
PROXY_LIST = [
    {"ip_port":"123.207.53.84:16816", "user_passwd":"morganna_mode_g:gcc22qxp"}, # 收费代理ip,有账号密码
    {"ip_port":"122.234.206.43:9000"} # 免费代理ip
]
```